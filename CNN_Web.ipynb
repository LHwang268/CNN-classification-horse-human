{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Web.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LHwang268/CNN-classification-horse-human/blob/main/CNN_Web.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import thư viện cần thiết\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "Nc-ycwqACfwQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjEauehNBMEA",
        "outputId": "8d609cd5-0339-4a58-fd39-abf66833e2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-07 16:55:01--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.107.128, 74.125.199.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.107.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘./horse-or-human.zip’\n",
            "\n",
            "./horse-or-human.zi 100%[===================>] 142.65M   189MB/s    in 0.8s    \n",
            "\n",
            "2022-04-07 16:55:01 (189 MB/s) - ‘./horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2022-04-07 16:55:02--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.128, 74.125.20.128, 74.125.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘./validation-horse-or-human.zip’\n",
            "\n",
            "./validation-horse- 100%[===================>]  10.95M  57.4MB/s    in 0.2s    \n",
            "\n",
            "2022-04-07 16:55:02 (57.4 MB/s) - ‘./validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get data from google api -> get .zip files\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O ./horse-or-human.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O ./validation-horse-or-human.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip files vừa lấy về, đẩy các ảnh vào folder train và valid\n",
        "train_folder = './horse-or-human'\n",
        "valid_folder = './valid-horse-or-human'\n",
        "\n",
        "!unzip -q horse-or-human.zip -d $train_folder\n",
        "!unzip -q validation-horse-or-human.zip -d $valid_folder"
      ],
      "metadata": {
        "id": "tRtnv0TyC10N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lấy ảnh, resize, chia batch\n",
        "train_data = ImageDataGenerator(rescale = 1/255).flow_from_directory(\n",
        "    train_folder,\n",
        "    target_size = (150, 150),\n",
        "    batch_size = 128,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3VmXOX0wHvE",
        "outputId": "b22ca920-0491-40c7-e6e0-6b29797b7da9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
        "    valid_folder,\n",
        "    target_size = (150, 150),\n",
        "    batch_size = 128,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FKHMulwyMBe",
        "outputId": "b830edba-667a-4bc0-c733-ff9d1dc15487"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 256 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Flatten"
      ],
      "metadata": {
        "id": "sMrDBDCq1xZ4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# may quá chời, CNN có thể xử lý được ảnh màu (3 channels), trước khi đưa qua lớp Dense, nhớ duỗi ảnh =))\n",
        "model = tf.keras.Sequential([tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150,150,3)),\n",
        "                             tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                             tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "                             tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                             tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "                             tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                             Flatten(),\n",
        "                             tf.keras.layers.Dense(265, activation='relu'),\n",
        "                             tf.keras.layers.Dense(2, activation='softmax')])"
      ],
      "metadata": {
        "id": "8ihVsu3-ylt2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCyskmFD1vb-",
        "outputId": "afb4dfcc-1627-49cc-b558-09ebe8d08520"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 128)       18560     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 36992)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 265)               9803145   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 532       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,970,269\n",
            "Trainable params: 9,970,269\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "InhUQQeY2AXK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, validation_data=valid_data, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lS3ADv62hPL",
        "outputId": "7bcebadb-7f00-4bb6-ad69-850ce6b33c7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "9/9 [==============================] - 53s 6s/step - loss: 1.3610 - acc: 0.5920 - val_loss: 0.5928 - val_acc: 0.5938\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - 51s 6s/step - loss: 0.4306 - acc: 0.8121 - val_loss: 0.3870 - val_acc: 0.8789\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - 51s 6s/step - loss: 0.2197 - acc: 0.9085 - val_loss: 0.7907 - val_acc: 0.8594\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - 55s 6s/step - loss: 0.1116 - acc: 0.9533 - val_loss: 1.8698 - val_acc: 0.8203\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - 53s 6s/step - loss: 0.0642 - acc: 0.9757 - val_loss: 1.9052 - val_acc: 0.8281\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - 54s 6s/step - loss: 0.0613 - acc: 0.9766 - val_loss: 2.0768 - val_acc: 0.8086\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - 54s 7s/step - loss: 0.0264 - acc: 0.9893 - val_loss: 1.8502 - val_acc: 0.8320\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - 51s 6s/step - loss: 0.0150 - acc: 0.9971 - val_loss: 1.6869 - val_acc: 0.8555\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - 51s 6s/step - loss: 0.0065 - acc: 0.9990 - val_loss: 1.4877 - val_acc: 0.8750\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - 51s 6s/step - loss: 0.0033 - acc: 1.0000 - val_loss: 1.8511 - val_acc: 0.8594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ab4a09e90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}